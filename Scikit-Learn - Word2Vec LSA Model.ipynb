{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Goal - Refactor and Improve the Gensim Word2Vec LSI NLP model using Scikit-learn, adhering to the CRISP-DM Process\n",
    "\n",
    "#LSI Overview: LSA is technique in natural language processing of analyzing relationships between a set of documents \n",
    "#and the terms they contain by producing a set of concepts related to the documents and terms. \n",
    "#i.e. LSA assumes that words that are close in meaning will occur in similar pieces of text \n",
    "\n",
    "#global variable that work as model parameters - adjust for model performance \"fine-tuning\"\n",
    "n_samples = 2000 #sample size\n",
    "n_features = 1000 #name/entity recgonition & group selection (vectors)\n",
    "n_components = 7 #themes\n",
    "n_top_words = 10 #words per theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder \"C:\\Users\\Bergmann\\AppData\\Local\\Temp\" will be used to save temporary dictionary and corpus.\n"
     ]
    }
   ],
   "source": [
    "#import libraries for data structures and Gensim Word2Vec API\n",
    "import os\n",
    "import tempfile\n",
    "TEMP_FOLDER = tempfile.gettempdir()\n",
    "print('Folder \"{}\" will be used to save temporary dictionary and corpus.'.format(TEMP_FOLDER))\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim #open-source achine learning framework\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "#data pre-processing tools from gensim package\n",
    "from gensim.parsing.preprocessing import strip_non_alphanum \n",
    "from gensim.parsing.preprocessing import strip_numeric\n",
    "from gensim.parsing.preprocessing import strip_punctuation\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
    "from gensim.parsing.preprocessing import strip_short\n",
    "#import scikit-learn & graphical libraries\n",
    "# Any results you write to the current directory are saved as output.\n",
    "from sklearn import preprocessing #data prep - module includes scaling, centering, normalization, binarization and imputation methods.\n",
    "from sklearn.feature_extraction import text #used for removing stop words and obtaining feature extraction from text\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns #Seaborn is a Python data visualization library based on matplotlib. \n",
    "sns.set(style=\"white\") #white background style for seaborn plots\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "\n",
    "#import sklearn libraries to NLP model bilding & validation\n",
    "from sklearn.metrics import accuracy_score  #used for model evaluation - https://scikit-learn.org/stable/modules/model_evaluation.html#model-evaluation\n",
    "from sklearn.feature_extraction.text import CountVectorizer  #bag-of-words vectorication for LDA model\n",
    "from sklearn.decomposition import LatentDirichletAllocation #model for NLP topic extraction, similar to gensim LDA\n",
    "from sklearn.datasets import make_multilabel_classification #create random test dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRISP-DM Task: Data Preparation\n",
      "Task 1: Read-in a text-based document, aka \"establishing the corpus\n",
      "Pandas File I/O Example - csv read\n",
      "        ID  Branch Interview Date Transaction Date Comment Type  \\\n",
      "0  4278642     355      2/24/2015        2/23/2015   Compliment   \n",
      "1  3329834     311      4/10/2014         4/9/2014   Compliment   \n",
      "2  4182303     353      1/14/2015        1/13/2015   Compliment   \n",
      "3  4228554     318       2/4/2015         2/3/2015   Compliment   \n",
      "4  3860433     351       9/4/2014         9/3/2014   Compliment   \n",
      "\n",
      "                                             Comment Follow-up  \\\n",
      "0  MY BANK is always good to me. I have banked wi...       NaN   \n",
      "1  MY BANK is the best for me. They help people w...       NaN   \n",
      "2  MY BANK has been 100 percent on top on any ban...       NaN   \n",
      "3  Absolutely no problems with them. Everything h...       NaN   \n",
      "4  Absolutely. They are efficient, courteous and ...       NaN   \n",
      "\n",
      "   Satisfaction Rating  \n",
      "0                   10  \n",
      "1                    8  \n",
      "2                    8  \n",
      "3                    9  \n",
      "4                    9  \n"
     ]
    }
   ],
   "source": [
    "#CRISP-DM Task: Data Preparation \n",
    "#import favorite text-based dataset for analysis using pandas dataframe - compatible w/scikit-learn\n",
    "def read_text(path):\n",
    "    print(\"Pandas File I/O Example - csv read\")\n",
    "    text=pd.read_csv(path) #import to pandas DataFrame\n",
    "    return text #return pandas dataframe type\n",
    "\n",
    "print('CRISP-DM Task: Data Preparation')\n",
    "print('Task 1: Read-in a text-based document, aka \"establishing the corpus')\n",
    "documents = read_text(r\"C:\\\\Python\\\\Data\\\\Text_Mining_Sample_CSV.csv\") #single-line text\n",
    "print(documents.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2: Preprocessing dataset, including stoplist, word frequencies & filters\n",
      "Task 2a: Remove punctuation, non-alphanumeric and numeric characters\n",
      "Preprocessing Corpus from pandas data frame\n"
     ]
    }
   ],
   "source": [
    "#preprocess data for use in text mining/NLP - refactored for pandas dataframe\n",
    "def preprocess_text(corpus,field_name = 'Comment'):\n",
    "    print(\"Preprocessing Corpus from pandas data frame\")\n",
    "    for index, row in corpus.iterrows():  #iterate through rows in dataframe\n",
    "        line = row['Comment'].strip('\\n')\n",
    "        line = strip_punctuation(line)\n",
    "        line = strip_non_alphanum(line)\n",
    "        line = strip_numeric(line)\n",
    "        line = strip_multiple_whitespaces(line)\n",
    "        line = strip_short(line)\n",
    "        #add cleaned text line to new dataframe\n",
    "        corpus.at[index,field_name] = line #set value at row/column in corpus dataframet            \n",
    "    return corpus\n",
    "\n",
    "print('Task 2: Preprocessing dataset, including stoplist, word frequencies & filters')\n",
    "print('Task 2a: Remove punctuation, non-alphanumeric and numeric characters')\n",
    "raw_corpus = preprocess_text(documents,field_name = 'Comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2b: remove english stopwords and add additional to remove from text document\n"
     ]
    }
   ],
   "source": [
    "print('Task 2b: remove english stopwords and add additional to remove from text document')\n",
    "#set stopword list - see here for set of english \"stop words\": https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/stop_words.py\n",
    "#scikit-learn uses a stoplist \"frozenset\" - immutable python set\" - to add additional stopwords, neded to union additional set  \n",
    "stop_words = text.ENGLISH_STOP_WORDS.union({\"have\", \"with\", \"are\", \"JBergmann\"})\n",
    "#print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2b: create a BOW vector for Latent-Dirichlet-Allocation (LDA) Model, using assigned stop words\n"
     ]
    }
   ],
   "source": [
    "print('Task 2b: create a BOW vector for Latent-Dirichlet-Allocation (LDA) Model, using assigned stop words')\n",
    "#Convert a collection of text documents to a matrix of token counts - \"bag-of-words\", unless otherwise specified\n",
    "bow_vector = CountVectorizer(stop_words=stop_words)\n",
    "#\"Comment\" column is the 6th column in the dataset - index \"5\" in dataframe\n",
    "value_list = [row[5] for row in raw_corpus.itertuples(index=False, name=None)]\n",
    "#print(value_list[0:3])\n",
    "#create term-document matrix and and place all relevant terms in vocabulary/dictionary\n",
    "bow = bow_vector.fit_transform(value_list)\n",
    "#dictionary stored in the vocabulary_ variable of the bow object \n",
    "#print(bow_vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRISP-DM Task: Data Understanding\n",
      "Task 1: print information about bow verctor and/or corpus\n",
      "109\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "print('CRISP-DM Task: Data Understanding')\n",
    "print('Task 1: print information about bow verctor and/or corpus')\n",
    "print(bow_vector.vocabulary_.get('atm')) #get dictionary index of \"get\" keyword\n",
    "counts = np.asarray(bow.sum(axis=0)) \n",
    "count_words = counts[0]\n",
    "freq = count_words[109] #get word count of all terms in dictionary, using retured \"get\" word id - ex atm = 109\n",
    "print(freq) #print wordcount frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2: use pandas and data viz libraries to explore & understand the columns and values in the text dataset\n",
      "        ID  Branch Interview Date Transaction Date Comment Type  \\\n",
      "0  4278642     355      2/24/2015        2/23/2015   Compliment   \n",
      "1  3329834     311      4/10/2014         4/9/2014   Compliment   \n",
      "2  4182303     353      1/14/2015        1/13/2015   Compliment   \n",
      "3  4228554     318       2/4/2015         2/3/2015   Compliment   \n",
      "4  3860433     351       9/4/2014         9/3/2014   Compliment   \n",
      "\n",
      "                                             Comment Follow-up  \\\n",
      "0  BANK always good have banked with them since w...       NaN   \n",
      "1  BANK the best for They help people when they r...       NaN   \n",
      "2  BANK has been percent top any bank They been a...       NaN   \n",
      "3  Absolutely problems with them Everything has b...       NaN   \n",
      "4  Absolutely They are efficient courteous and he...       NaN   \n",
      "\n",
      "   Satisfaction Rating  \n",
      "0                   10  \n",
      "1                    8  \n",
      "2                    8  \n",
      "3                    9  \n",
      "4                    9  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1193 entries, 0 to 1192\n",
      "Data columns (total 8 columns):\n",
      "ID                     1193 non-null int64\n",
      "Branch                 1193 non-null int64\n",
      "Interview Date         1193 non-null object\n",
      "Transaction Date       1193 non-null object\n",
      "Comment Type           1193 non-null object\n",
      "Comment                1193 non-null object\n",
      "Follow-up              236 non-null object\n",
      "Satisfaction Rating    1193 non-null int64\n",
      "dtypes: int64(3), object(5)\n",
      "memory usage: 74.6+ KB\n",
      "None\n",
      "ID                       0\n",
      "Branch                   0\n",
      "Interview Date           0\n",
      "Transaction Date         0\n",
      "Comment Type             0\n",
      "Comment                  0\n",
      "Follow-up              957\n",
      "Satisfaction Rating      0\n",
      "dtype: int64\n",
      "                 ID       Branch  Satisfaction Rating\n",
      "count  1.193000e+03  1193.000000          1193.000000\n",
      "mean   4.297436e+06   295.588433             7.985750\n",
      "std    4.774175e+05    86.540331             2.252656\n",
      "min    3.276177e+06   107.000000             1.000000\n",
      "25%    3.954094e+06   310.000000             8.000000\n",
      "50%    4.322839e+06   320.000000             9.000000\n",
      "75%    4.720735e+06   352.000000             9.000000\n",
      "max    5.081369e+06   401.000000            10.000000\n",
      "  Comment Type  count\n",
      "0        Alert    236\n",
      "1   Compliment    957\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x26614b06a90>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Task 2: use pandas and data viz libraries to explore & understand the columns and values in the text dataset')\n",
    "print(documents.head())  #get first 5 observations in pandas dataframe\n",
    "# Check data types for each variable\n",
    "print(documents.info())\n",
    "#assess data quailty - null values \n",
    "print(documents.isnull().sum())\n",
    "#describe dataset values \n",
    "print(documents.describe())\n",
    "#view histograp of categorical variables \n",
    "#summarize & plot pandas column using \"groupby\" function\n",
    "summary = documents.groupby(['Comment Type'])[\"ID\"].count().reset_index(name=\"count\")\n",
    "print(summary)\n",
    "y = summary['count']\n",
    "x = summary['Comment Type']   #iterate list to transfor dates for graphical use\n",
    "data = pd.DataFrame({'Freq':y, 'Comment Type':x}).set_index(x) \n",
    "data.plot(kind='bar')\n",
    "#plt.show()  #uncomment to show - otherwise holds execution until closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRISP-DM Task: Model Building\n",
      "Fitting LDA models with tf features, n_samples=2000 and n_features=1000...\n",
      "\n",
      "Topics in LDA model:\n",
      "Topic #0: drive inside window service customer light alley mentioned apology assuming\n",
      "Topic #1: caught atms fraud needs area winamac eric west exceptional team\n",
      "Topic #2: statements sent paper payment error letter canada wasn placed charged\n",
      "Topic #3: fault funds send total deposited fee clicked able hold following\n",
      "Topic #4: bank like know account service friendly just good don people\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('CRISP-DM Task: Model Building')\n",
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "lda.fit(bow)\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "bow_feature_names = bow_vector.get_feature_names()\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "print_top_words(lda, bow_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRISP-DM Task: Model Evaluation\n",
      "Step1 : get test topics then score them vs. current LDA model\n",
      "-22711.2824292\n"
     ]
    }
   ],
   "source": [
    "print('CRISP-DM Task: Model Evaluation')\n",
    "print('Step1 : get test topics then score them vs. current LDA model')\n",
    "train, test = train_test_split(list(documents['Comment'].values), test_size = 0.2)\n",
    "test_vector = bow_vector.fit_transform(test)\n",
    "test_lda = lda.fit(test_vector)  #fit text vector within existing model - transpose to work\n",
    "#Calculate approximate log-likelihood as score.\n",
    "print(test_lda.score(test_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Use formal model evaluation stats \"perplexity\"\n",
      "2660.40661702\n"
     ]
    }
   ],
   "source": [
    "print('Step 2: Use formal model evaluation stats \"perplexity\"') \n",
    "# create test/train text documents to evaluate model \n",
    "vectoriser = CountVectorizer(stop_words = 'english', max_features=500)  #max features must be less that \"n_features\" variable!\n",
    "doc_train = vectoriser.fit_transform(train)\n",
    "features = vectoriser.get_feature_names()\n",
    "doc_test = vectoriser.fit_transform(test)\n",
    "news_lda = lda.fit(doc_train)\n",
    "print(news_lda.perplexity(doc_test)) # lower the perplexity, the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 3: Compare vs. Gensim word2vec topic output\n",
      "CRISP-DM Task: Model Deployment\n"
     ]
    }
   ],
   "source": [
    "print('Task 3: Compare vs. Gensim word2vec topic output')\n",
    "#class exercise - what is the difference? \n",
    "\n",
    "print('CRISP-DM Task: Model Deployment')\n",
    "#gensim contains ablility to save and update models with future iterations\n",
    "#lecture is TBD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
