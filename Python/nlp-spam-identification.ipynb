{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import re\nimport string\nimport jieba\nimport gensim\nimport os\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn import metrics\nprint(os.listdir(\"../input/spam identification/dict\"))\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"def get_stop_words():\n    stopword_list = []\n    with open(\"../input/spam identification/dict/stop_words.utf8\") as f:\n        lines = f.readlines()\n        for line in lines:\n            line = line.strip()\n            stopword_list.append(line)\n    return stopword_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef26a831e8d4aa0b498e9e6cfd079aa6693d74be"},"cell_type":"code","source":"def tokenize_text(text):\n    tokens = jieba.cut(text)\n    tokens = [token.strip() for token in tokens]\n    return tokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0709a211ce6c4ab450951aea01087d31e9e5d10e"},"cell_type":"code","source":"def remove_stopwords(text):\n    tokens = tokenize_text(text)\n    stopword_list = get_stop_words()\n    filtered_tokens = [token for token in tokens if token not in stopword_list]\n    filtered_text = ''.join(filtered_tokens)\n    return filtered_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ce5c5d79d7c8c52b8bfd05bb77ce6ed55364f98"},"cell_type":"code","source":"remove_stopwords(\"今天的天气很好，有点紧张\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dbe8a5d4fffedef2e49cf9ae0f5d89463cc7fd37"},"cell_type":"code","source":"def remove_special_characters(text):\n    tokens = tokenize_text(text)\n    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n    filtered_tokens = filter(None,[re.sub(pattern=pattern,repl=\"\",string=token) for token in tokens])\n    filtered_text = ''.join(filtered_tokens)\n    return filtered_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17981b0f76fe4cc8afc618c5b287a498f08a2edb"},"cell_type":"code","source":"remove_special_characters(\"今天的天气很好，有点紧张#()10\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b6c2dcf174b496f3efadf227fd63042d8aaa9882"},"cell_type":"code","source":"def normalize_corpus(corpus):\n    normalized_corpus = []\n    for text in corpus:\n        text = remove_special_characters(text)\n        text = remove_stopwords(text)\n        normalized_corpus.append(text)\n    return normalized_corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db8e6d7c29d5f0f41cbda1a867729fae106099cb"},"cell_type":"code","source":"def bow_extractor(corpus, ngram_range=(1, 1)):\n    vectorizer = CountVectorizer(min_df=1, ngram_range=ngram_range)\n    features = vectorizer.fit_transform(corpus)\n    return vectorizer, features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0eeccb63c2000420025c3dc79f04a01821874d7b"},"cell_type":"code","source":"def tfidf_transformer(bow_matrix):\n    transformer = TfidfTransformer(norm='l2',\n                                   smooth_idf=True,\n                                   use_idf=True)\n    tfidf_matrix = transformer.fit_transform(bow_matrix)\n    return transformer, tfidf_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17fb45245b69af0c1b7bd3fa7f5a250da8ae44ba"},"cell_type":"code","source":"def tfidf_extractor(corpus, ngram_range=(1, 1)):\n    vectorizer = TfidfVectorizer(min_df=1,\n                                 norm='l2',\n                                 smooth_idf=True,\n                                 use_idf=True,\n                                 ngram_range=ngram_range)\n    features = vectorizer.fit_transform(corpus)\n    return vectorizer, features\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"def2facddbef6accc803613740bcd3f5e6a211e7"},"cell_type":"code","source":"def get_data():\n    '''\n    获取数据\n    :return: 文本数据，对应的labels\n    '''\n    with open(\"../input/spam identification/data/ham_data.txt\", encoding=\"utf8\") as ham_f, open(\"../input/spam identification/data/spam_data.txt\", encoding=\"utf8\") as spam_f:\n        ham_data = ham_f.readlines()\n        spam_data = spam_f.readlines()\n        \n        ham_label = np.ones(len(ham_data)).tolist()\n        spam_label = np.zeros(len(spam_data)).tolist()\n        corpus = ham_data + spam_data\n        labels = ham_label + spam_label\n    return corpus, labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4e7ba77c5d3ca210fab3d81c2b7339feb3d7c52"},"cell_type":"code","source":"def prepare_datasets(corpus, labels, test_data_proportion=0.3):\n    '''\n    :param corpus: 文本数据\n    :param labels: label数据\n    :param test_data_proportion:测试数据占比 \n    :return: 训练数据,测试数据，训练label,测试label\n    '''\n    train_X, test_X, train_Y, test_Y = train_test_split(corpus, labels,\n                                                        test_size=test_data_proportion, random_state=42)\n    return train_X, test_X, train_Y, test_Y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e6d2ddd8bf7e7d9f1d4bd8b91b09f277028bc442"},"cell_type":"code","source":"def remove_empty_docs(corpus, labels):\n    filtered_corpus = []\n    filtered_labels = []\n    for doc, label in zip(corpus, labels):\n        if doc.strip():\n            filtered_corpus.append(doc)\n            filtered_labels.append(label)\n\n    return filtered_corpus, filtered_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"23c094ecb05f99e0d7197ce4c36b28b942406f04"},"cell_type":"code","source":"def get_metrics(true_labels, predicted_labels):\n    print('准确率:', np.round(\n        metrics.accuracy_score(true_labels,\n                               predicted_labels),\n        2))\n    print('精度:', np.round(\n        metrics.precision_score(true_labels,\n                                predicted_labels,\n                                average='weighted'),\n        2))\n    print('召回率:', np.round(\n        metrics.recall_score(true_labels,\n                             predicted_labels,\n                             average='weighted'),\n        2))\n    print('F1得分:', np.round(\n        metrics.f1_score(true_labels,\n                         predicted_labels,\n                         average='weighted'),\n        2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1631224887f96a531c73566f211642297b59ef32"},"cell_type":"code","source":"def train_predict_evaluate_model(classifier,\n                                 train_features, train_labels,\n                                 test_features, test_labels):\n    # build model\n    classifier.fit(train_features, train_labels)\n    # predict using model\n    predictions = classifier.predict(test_features)\n    # evaluate model prediction performance\n    get_metrics(true_labels=test_labels,\n                predicted_labels=predictions)\n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ffd7037af79e3b112903ac89f3c3abb2c11bb699"},"cell_type":"code","source":"def main():\n    corpus, labels = get_data()  # 获取数据集\n\n    print(\"总的数据量:\", len(labels))\n\n    corpus, labels = remove_empty_docs(corpus, labels)\n\n    print('样本之一:', corpus[10])\n    print('样本的label:', labels[10])\n    label_name_map = [\"垃圾邮件\", \"正常邮件\"]\n    print('实际类型:', label_name_map[int(labels[10])], label_name_map[int(labels[5900])])\n    \n    #对数据进行划分\n    train_corpus, test_corpus, train_labels, test_labels = prepare_datasets(corpus,\n                                                                            labels,\n                                                                            test_data_proportion=0.3)\n    #进行归一化\n    norm_train_corpus = normalize_corpus(train_corpus)\n    norm_test_corpus = normalize_corpus(test_corpus)\n    \n    # 词袋模型特征\n    bow_vectorizer, bow_train_features = bow_extractor(norm_train_corpus)\n    bow_test_features = bow_vectorizer.transform(norm_test_corpus)\n\n    # tfidf 特征\n    tfidf_vectorizer, tfidf_train_features = tfidf_extractor(norm_train_corpus)\n    tfidf_test_features = tfidf_vectorizer.transform(norm_test_corpus)\n\n    # tokenize documents\n    tokenized_train = [jieba.lcut(text)\n                       for text in norm_train_corpus]\n    #print(tokenized_train[2:10])\n    tokenized_test = [jieba.lcut(text)\n                      for text in norm_test_corpus]\n    \n    # build word2vec 模型\n    model = gensim.models.Word2Vec(tokenized_train,\n                                   size=500,\n                                   window=100,\n                                   min_count=30,\n                                   sample=1e-3)\n\n    from sklearn.naive_bayes import MultinomialNB\n    from sklearn.linear_model import SGDClassifier\n    from sklearn.linear_model import LogisticRegression\n    mnb = MultinomialNB()\n    svm = SGDClassifier(loss='hinge', n_iter=100)\n    lr = LogisticRegression()\n\n    print(\"\\n\")\n    # 基于词袋模型的多项朴素贝叶斯\n    print(\"基于词袋模型特征的贝叶斯分类器\")\n    mnb_bow_predictions = train_predict_evaluate_model(classifier=mnb,\n                                                       train_features=bow_train_features,\n                                                       train_labels=train_labels,\n                                                       test_features=bow_test_features,\n                                                       test_labels=test_labels)\n\n    # 基于词袋模型特征的逻辑回归\n    print(\"基于词袋模型特征的逻辑回归\")\n    lr_bow_predictions = train_predict_evaluate_model(classifier=lr,\n                                                      train_features=bow_train_features,\n                                                      train_labels=train_labels,\n                                                      test_features=bow_test_features,\n                                                      test_labels=test_labels)\n\n    # 基于词袋模型的支持向量机方法\n    print(\"基于词袋模型的支持向量机\")\n    svm_bow_predictions = train_predict_evaluate_model(classifier=svm,\n                                                       train_features=bow_train_features,\n                                                       train_labels=train_labels,\n                                                       test_features=bow_test_features,\n                                                       test_labels=test_labels)\n\n\n    # 基于tfidf的多项式朴素贝叶斯模型\n    print(\"基于tfidf的贝叶斯模型\")\n    mnb_tfidf_predictions = train_predict_evaluate_model(classifier=mnb,\n                                                         train_features=tfidf_train_features,\n                                                         train_labels=train_labels,\n                                                         test_features=tfidf_test_features,\n                                                         test_labels=test_labels)\n    # 基于tfidf的逻辑回归模型\n    print(\"基于tfidf的逻辑回归模型\")\n    lr_tfidf_predictions=train_predict_evaluate_model(classifier=lr,\n                                                         train_features=tfidf_train_features,\n                                                         train_labels=train_labels,\n                                                         test_features=tfidf_test_features,\n                                                         test_labels=test_labels)\n\n\n    # 基于tfidf的支持向量机模型\n    print(\"基于tfidf的支持向量机模型\")\n    svm_tfidf_predictions = train_predict_evaluate_model(classifier=svm,\n                                                         train_features=tfidf_train_features,\n                                                         train_labels=train_labels,\n                                                         test_features=tfidf_test_features,\n                                                         test_labels=test_labels)\n    \n    \n    num = 0\n    for document, label, predicted_label in zip(test_corpus, test_labels, svm_tfidf_predictions):\n        print('邮件类型:', label_name_map[int(label)])\n        print('预测的邮件类型:', label_name_map[int(predicted_label)])\n        print('文本:-')\n        print(re.sub('\\n', ' ', document))\n        print(\"\\n\")\n        num+=1\n        \n        if num == 10:\n            break\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a10b4d354ff6b290a63eaa4b59c8c47d3c33bb30"},"cell_type":"code","source":"main()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ec4fc5d26c8d61f7efda917116812ed7ee1f7bb"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77606854945c5545699e0b9509630888d3b181dc"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4cb2261d8e3d8e6a805f758820e1fd22c5fb4dd6"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f932ec13a30e17a98ace816d9ef44860e2b6a04"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f0809d651b0badd0beeaae95373425fe7fc5cfa"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}