{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit learn: Working with text data\n",
    "# tutorial: https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "# github source: https://github.com/scikit-learn/scikit-learn/tree/master/doc/tutorial/text_analytics\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "\n",
    "#data/model prep - module for preprocessing & model prep - includes scaling, centering, normalization, binarization and imputation methods.\n",
    "from sklearn import preprocessing \n",
    "#used for removing stop words and obtaining feature extraction from text\n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.feature_extraction.text import CountVectorizer  #bag-of-words vectorication for LDA model\n",
    "\n",
    "#import sklearn libraries to NLP model bilding & validation\n",
    "from sklearn.metrics import accuracy_score  #used for model evaluation - https://scikit-learn.org/stable/modules/model_evaluation.html#model-evaluation\n",
    "from sklearn.decomposition import LatentDirichletAllocation #model for NLP topic extraction, similar to gensim LDA\n",
    "from sklearn.datasets import make_multilabel_classification #create random test dataset\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: sd345@city.ac.uk (Michael Collier)\n",
      "Subject: Converting images to HP LaserJet III?\n",
      "Nntp-Posting-Host: hampton\n",
      "Organization: The City University\n",
      "Lines: 14\n",
      "\n",
      "Does anyone know of a good way (standard PC application/PD utility) to\n",
      "convert tif/img/tga files into LaserJet III format.  We would also like to\n",
      "do the same, converting to HPGL (HP plotter) files.\n",
      "\n",
      "Please email any response.\n",
      "\n",
      "Is this the correct group?\n",
      "\n",
      "Thanks in advance.  Michael.\n",
      "-- \n",
      "Michael Collier (Programmer)                 The Computer Unit,\n",
      "Email: M.P.Collier@uk.ac.city                The City University,\n",
      "Tel: 071 477-8000 x3769                      London,\n",
      "Fax: 071 477-8565                            EC1V 0HB.\n"
     ]
    }
   ],
   "source": [
    "# Loading the 20 newsgroups dataset - collection of approximately 20,000 newsgroup documents, \n",
    "#partitioned (nearly) evenly across 20 different newsgroups\n",
    "\n",
    "#partial dataset with only 4 categories out of the 20 available in the dataset\n",
    "categories = ['alt.atheism', 'soc.religion.christian',\n",
    "               'comp.graphics', 'sci.med']\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train',\n",
    "    categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Understanding/Exploration - Tableau + Summary Stats/Graphs\n",
    "#Let's print the first lines of the first loaded file:\n",
    "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "#Data Prep - Preprocessing - Text Cleaning, Model Prep - Vectorization\n",
    "#Text preprocessing, tokenizing and filtering of stopwords are all included in CountVectorizer, \n",
    "#which builds a dictionary of features and transforms documents to feature vectors:\n",
    "\n",
    "#Remove stop words from text - for additioal text extraction options, see:  \n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "count_vect = CountVectorizer(stop_words={'english','science','math','beer'})\n",
    "                                         #,'algorithm'})\n",
    "\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data) #create model building \"Train\" vectorized dataset\n",
    "#CountVectorizer supports counts of N-grams of words or consecutive characters. \n",
    "#Once fitted, the vectorizer has built a dictionary of feature indices\n",
    "print(count_vect.vocabulary_.get(u'algorithm')) #count # of occurences of word in documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Prep - Text Vectorization\n",
    "#Alternative - term frequency (tf) vectorization - Word Counts\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "\n",
    "#Alternative - term frequency (tf-idf) vectorization - Term Frequency times Inverse Document Frequency - Dimensionality Reduction\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Building/Training - Classification - predict the category of a post using naïve Bayes classifier for word counts\n",
    "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'God is love' => soc.religion.christian\n",
      "'OpenGL on the GPU is fast' => comp.graphics\n"
     ]
    }
   ],
   "source": [
    "# make predictions  - outcome on a new document we need to extract the features using almost the same feature \n",
    "# extracting chain as before. The difference is that we call transform instead of fit_transform on the transformers, \n",
    "# since they have already been fit to the training set - using MultinomialNB() function\n",
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "     print('%r => %s' % (doc, twenty_train.target_names[category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order to make the vectorizer => transformer => classifier easier to work with, scikit-learn provides a \n",
    "#Pipeline class that behaves like a compound classifier:\n",
    "from sklearn.pipeline import Pipeline\n",
    "text_clf1 = Pipeline([\n",
    "     ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "     ('clf', MultinomialNB()),  #Naive Bays Classifier\n",
    "]);\n",
    "\n",
    "text_clf2 = Pipeline([\n",
    "     ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                          alpha=1e-3, random_state=42,\n",
    "                           max_iter=5, tol=None)), #SVM classifier\n",
    "]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB Model Classification %: \n",
      "0.8348868175765646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Model Classification %: \n",
      "0.9127829560585885\n"
     ]
    }
   ],
   "source": [
    "# Model Selection - summarize the fit of the model\n",
    "import numpy as np\n",
    "twenty_test = fetch_20newsgroups(subset='test',\n",
    "    categories=categories, shuffle=True, random_state=42)\n",
    "docs_test = twenty_test.data\n",
    "\n",
    "#run naive bayes classifier pipeline and assess fit\n",
    "text_clf1.fit(twenty_train.data, twenty_train.target)  \n",
    "predicted = text_clf1.predict(docs_test)\n",
    "print(\"NB Model Classification %: \")\n",
    "print(np.mean(predicted == twenty_test.target)) \n",
    "\n",
    "#Note: We achieved 83.5% accuracy. Let’s see if we can do better with a linear support vector machine (SVM), \n",
    "#which is widely regarded as one of the best text classification algorithms \n",
    "\n",
    "#run SVM classifier\n",
    "text_clf2.fit(twenty_train.data, twenty_train.target)  \n",
    "predicted = text_clf2.predict(docs_test)\n",
    "print(\"SVM Model Classification %: \")\n",
    "print(np.mean(predicted == twenty_test.target)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.95      0.81      0.87       319\n",
      "         comp.graphics       0.88      0.97      0.92       389\n",
      "               sci.med       0.94      0.90      0.92       396\n",
      "soc.religion.christian       0.90      0.95      0.93       398\n",
      "\n",
      "             micro avg       0.91      0.91      0.91      1502\n",
      "             macro avg       0.92      0.91      0.91      1502\n",
      "          weighted avg       0.92      0.91      0.91      1502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#detailed model performance results \n",
    "print(metrics.classification_report(twenty_test.target, predicted,\n",
    "     target_names=twenty_test.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
