{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder \"C:\\Users\\Bergmann\\AppData\\Local\\Temp\" will be used to save temporary dictionary and corpus.\n"
     ]
    }
   ],
   "source": [
    "#Goal - Implement and Evaluate the LSI NLP model, using the CRISP-DM Process\n",
    "\n",
    "#LSI Overview: LSI is technique in natural language processing of analyzing relationships between a set of documents \n",
    "#and the terms they contain by producing a set of concepts related to the documents and terms. \n",
    "#i.e. LSA assumes that words that are close in meaning will occur in similar pieces of text \n",
    "\n",
    "#import libraries for data structures and Gensim Word2Vec API\n",
    "import os\n",
    "import tempfile\n",
    "TEMP_FOLDER = tempfile.gettempdir()\n",
    "print('Folder \"{}\" will be used to save temporary dictionary and corpus.'.format(TEMP_FOLDER))\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim #open-source achine learning framework\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim.parsing.preprocessing import strip_non_alphanum \n",
    "from gensim.parsing.preprocessing import strip_numeric\n",
    "from gensim.parsing.preprocessing import strip_punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRISP-DM Task: Data Preparation\n",
      "Task 1: Read-in a text-based document, aka \"establishing the corpus\n",
      "Python File I/O Example - text Read\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'format' is an invalid keyword argument for this function",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-69325a4228e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'CRISP-DM Task: Data Preparation'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Task 1: Read-in a text-based document, aka \"establishing the corpus'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdocuments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"C:\\\\Python\\\\Data\\\\Text8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#single-line text\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-69325a4228e2>\u001b[0m in \u001b[0;36mread_text\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Python File I/O Example - text Read\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'format' is an invalid keyword argument for this function"
     ]
    }
   ],
   "source": [
    "#CRISP-DM Task: Data Preparation \n",
    "#import favorite text dataset for analysis \n",
    "def read_text(path):\n",
    "    print(\"Python File I/O Example - text Read\")\n",
    "    with open(path, \"r\", encoding='utf-8') as f:\n",
    "    \tline = f.readlines()\n",
    "    return line\n",
    "\n",
    "print('CRISP-DM Task: Data Preparation')\n",
    "print('Task 1: Read-in a text-based document, aka \"establishing the corpus')\n",
    "documents = read_text(r\"C:\\\\Python\\\\Data\\\\Text8\") #single-line text\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2: Preprocessing dataset, including stoplist, word frequencies & filters\n",
      "Task 2a: Remove punctuation, non-alphanumeric and numeric characters\n",
      "Preprocessing Corpus from list data structure\n"
     ]
    }
   ],
   "source": [
    "print('Task 2: Preprocessing dataset, including stoplist, word frequencies & filters')\n",
    "print('Task 2a: Remove punctuation, non-alphanumeric and numeric characters')\n",
    "#preprocess data for use in text mining/NLP\n",
    "def preprocess_text(corpus=[]):\n",
    "    print(\"Preprocessing Corpus from list data structure\")\n",
    "    for i, val in enumerate(corpus):  #iterate through list\n",
    "\t    corpus[i] = corpus[i].strip('\\n')\n",
    "\t    corpus[i] = strip_punctuation(corpus[i])\n",
    "\t    corpus[i] = strip_non_alphanum(corpus[i])\n",
    "\t    corpus[i] = strip_numeric(corpus[i])\n",
    "    return corpus\n",
    "\n",
    "raw_corpus = preprocess_text(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2b: Remove words in stoplist and Lowercase each document\n"
     ]
    }
   ],
   "source": [
    "print('Task 2b: Remove words in stoplist and Lowercase each document')\n",
    "#stoplist = set('for a of the and to in i they it my me that have with are was is t s ve he re is'.split())\n",
    "stoplist = set('for a of the and to in i they it my me that have with are was'.split())\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "          for document in raw_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2c: create a list list of non-distinct parsed words from doc\n"
     ]
    }
   ],
   "source": [
    "print('Task 2c: create a list list of non-distinct parsed words from doc') \n",
    "# Count word frequencies\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2d: Only keep words that appear more than once\n"
     ]
    }
   ],
   "source": [
    "print('Task 2d: Only keep words that appear more than once')\n",
    "processed_corpus = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "#print(processed_corpus) #long list of distinct words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data understanding\n",
    "processed_corpus.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 3: Transform Data - Create dictionary/term-document matrix\n",
      "{'computer': 0, 'human': 1, 'interface': 2, 'response': 3, 'survey': 4, 'system': 5, 'time': 6, 'user': 7, 'eps': 8, 'trees': 9, 'graph': 10, 'minors': 11}\n"
     ]
    }
   ],
   "source": [
    "print('CRISP-DM Task: Model Building')\n",
    "print('Task 1: Transform Data - Create dictionary/term-document matrix')\n",
    "#associate each word in the processed corpus with a unique integer ID, using the gensim.corpora.Dictionary class. \n",
    "#This dictionary defines the vocabulary of all words that our processing knows about.\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRISP-DM Task: Model Building\n",
      "Task 1: convert training document by vectorizing processed corpus into \"bag-of-words\" vectors,using dictionary data structure\n"
     ]
    }
   ],
   "source": [
    "#To infer the latent structure in our training corpus we need a way to represent documents\n",
    "#that we can manipulate mathematically. One approach is to represent each document as a vector. \n",
    "print('Task 2: convert training document by vectorizing processed corpus into \"bag-of-words\" vectors,' \n",
    "\t  + 'using dictionary data structure')\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2: Train Model by Applying NLP methodology (LSI Model) to vectorized \"bag of words\" corpus\n",
      "output Lsi Training Model\n",
      "[(0, '0.644*\"system\" + 0.404*\"user\" + 0.301*\"eps\" + 0.265*\"time\" + 0.265*\"response\" + 0.240*\"computer\" + 0.221*\"human\" + 0.206*\"survey\" + 0.198*\"interface\" + 0.036*\"graph\"'), (1, '0.623*\"graph\" + 0.490*\"trees\" + 0.451*\"minors\" + 0.274*\"survey\" + -0.167*\"system\" + -0.141*\"eps\" + -0.113*\"human\" + 0.107*\"response\" + 0.107*\"time\" + -0.072*\"interface\"'), (2, '0.426*\"response\" + 0.426*\"time\" + -0.361*\"system\" + 0.338*\"user\" + -0.330*\"eps\" + -0.289*\"human\" + -0.231*\"trees\" + -0.223*\"graph\" + 0.178*\"survey\" + 0.164*\"computer\"'), (3, '0.595*\"computer\" + 0.552*\"interface\" + 0.415*\"human\" + -0.333*\"system\" + -0.188*\"eps\" + -0.099*\"user\" + -0.074*\"time\" + -0.074*\"response\" + 0.032*\"survey\" + -0.025*\"trees\"'), (4, '0.594*\"trees\" + -0.537*\"survey\" + 0.332*\"user\" + -0.300*\"minors\" + 0.282*\"interface\" + -0.159*\"system\" + 0.115*\"eps\" + -0.107*\"computer\" + -0.106*\"human\" + 0.080*\"response\"'), (5, '0.496*\"interface\" + -0.392*\"trees\" + 0.385*\"user\" + -0.341*\"human\" + 0.277*\"minors\" + 0.272*\"eps\" + -0.255*\"computer\" + -0.207*\"system\" + -0.170*\"response\" + -0.170*\"time\"'), (6, '-0.523*\"human\" + 0.467*\"survey\" + -0.339*\"minors\" + 0.302*\"computer\" + 0.288*\"trees\" + -0.283*\"time\" + -0.283*\"response\" + 0.166*\"system\" + -0.160*\"graph\" + 0.070*\"interface\"'), (7, '0.681*\"graph\" + -0.678*\"minors\" + -0.255*\"trees\" + -0.062*\"computer\" + 0.060*\"human\" + 0.036*\"survey\" + -0.034*\"system\" + 0.019*\"eps\" + 0.016*\"response\" + 0.016*\"time\"'), (8, '-0.579*\"survey\" + 0.492*\"computer\" + -0.407*\"human\" + 0.271*\"system\" + 0.232*\"graph\" + -0.225*\"trees\" + 0.183*\"minors\" + -0.165*\"eps\" + -0.109*\"interface\" + -0.054*\"time\"')]\n"
     ]
    }
   ],
   "source": [
    "print('Task 3: Train Model by Applying NLP methodology (LSI Model) to vectorized \"bag of words\" corpus')\n",
    "#LSI is being used in a variety of information retrieval and text processing applications, \n",
    "#although its primary application has been for concept searching and automated document categorization\n",
    "modelLsi = models.LsiModel(bow_corpus, id2word=dictionary, num_topics=10) #initalize & train model on vectorized data\n",
    "print('output Lsi Training Model')\n",
    "#Cosine measure returns similarities in the range <-1, 1> (the greater, the more similar).\n",
    "print(modelLsi.print_topics(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRISP-DM Task: Model Evaluation\n",
      "Task 1: Test model by creating a topic via a Python list of keywords, then vectorize into a \"bag of words\" vector\n",
      "Test Theme: human computer interaction\n",
      "[(0, 0.46182100453271607), (1, -0.070027665279000589), (2, -0.12452907551899114), (3, 1.0097125584438551), (4, -0.21303040605626802), (5, -0.59593845338206597), (6, -0.22041753546094417), (7, -0.0018778773554750036), (8, 0.085766854949955507)]\n",
      "Task 2: formal tests of model accuracy\n",
      "CRISP-DM Task: Model Deployment\n"
     ]
    }
   ],
   "source": [
    "print('CRISP-DM Task: Model Evaluation')\n",
    "#key for text analytics is interpretability - does it make sense?  \n",
    "print('Task 1: Test model by creating a topic via a Python list of keywords, then vectorize into a \"bag of words\" vector')\n",
    "new_doc = \"human computer interaction\"\n",
    "#new_doc = \"branch bank service\"\n",
    "print('Test Theme: ' + new_doc)\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "#Calling modelLsi[new_vec] creates a wrapper around the old corpus document stream\n",
    "modelLsi_test = modelLsi[new_vec] #use \"testing\" data to transform the \"new\" document vector\n",
    "print(modelLsi_test) #if model isn't high quality, continue to iterate\n",
    "\n",
    "print('Task 2: formal tests of model accuracy')\n",
    "#include if find - can also be done in Dataiku\n",
    "\n",
    "print('CRISP-DM Task: Model Deployment')\n",
    "#gensim contains ablility to save and update models with future iterations\n",
    "#lecture is TBD - Pair with Data-Scikit & Dataiku"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
