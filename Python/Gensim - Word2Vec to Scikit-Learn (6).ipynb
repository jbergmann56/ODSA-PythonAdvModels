{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder \"C:\\Users\\JTBVEN~1\\AppData\\Local\\Temp\" will be used to save temporary dictionary and corpus.\n"
     ]
    }
   ],
   "source": [
    "#Gensim Docs: http://pandas.pydata.org/pandas-docs/stable/\n",
    "#Description: https://en.wikipedia.org/wiki/Word2vec\n",
    "#Tutorials/Code Example:  https://github.com/RaRe-Technologies/gensim/blob/develop/gensim%20Quick%20Start.ipynb\n",
    "#Library Purpose: Gensim Word2vec is a two-layer neural net that processes text.\n",
    "#Its input is a text corpus and its output is a set of vectors: feature vectors for words in that corpus\n",
    "#it can be applied just as well to genes, code, likes, playlists, social media graphs and \n",
    "#other verbal or symbolic series in which patterns may be discerned.\n",
    "\n",
    "\"\"\"Basic word2vec example.\"\"\"\n",
    "#open-source achine learning framework\n",
    "import gensim \n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim.test.utils import get_tmpfile\n",
    "#data pre-processing tools from gensim package\n",
    "from gensim.parsing.preprocessing import preprocess_string \n",
    "from gensim.parsing.preprocessing import remove_stopwords \n",
    "from gensim.parsing.preprocessing import strip_non_alphanum \n",
    "from gensim.parsing.preprocessing import strip_numeric\n",
    "from gensim.parsing.preprocessing import strip_punctuation\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
    "from gensim.parsing.preprocessing import stem_text #transform to lowercase then stem\n",
    "from gensim.summarization import summarize\n",
    "from gensim.summarization import keywords\n",
    "#other packages\n",
    "from collections import defaultdict #high performace dictionary - Python Standard Library\n",
    "\n",
    "#import scikit-learn & graphical libraries\n",
    "# Any results you write to the current directory are saved as output.\n",
    "from sklearn import preprocessing #data prep - module includes scaling, centering, normalization, binarization and imputation methods.\n",
    "from sklearn.feature_extraction import text #used for removing stop words and obtaining feature extraction from text\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns #Seaborn is a Python data visualization library based on matplotlib. \n",
    "sns.set(style=\"white\") #white background style for seaborn plots\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "from sklearn import preprocessing #data prep - module includes scaling, centering, normalization, binarization and imputation methods.\n",
    "from sklearn.feature_extraction import text #used for removing stop words and obtaining feature extraction from text\n",
    "\n",
    "#import sklearn libraries for NLP prep, model bilding & validation steps - for use in Dataiku\n",
    "from sklearn.metrics import accuracy_score  #used for model evaluation - https://scikit-learn.org/stable/modules/model_evaluation.html#model-evaluation\n",
    "from sklearn.feature_extraction.text import CountVectorizer  #bag-of-words vectorication for LDA model\n",
    "from sklearn.decomposition import LatentDirichletAllocation #model for NLP topic extraction, similar to gensim LDA\n",
    "from sklearn.datasets import make_multilabel_classification #create random test dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#import libraries for data structures and Gensim Word2Vec API\n",
    "import os\n",
    "import tempfile\n",
    "TEMP_FOLDER = tempfile.gettempdir()\n",
    "print('Folder \"{}\" will be used to save temporary dictionary and corpus.'.format(TEMP_FOLDER))\n",
    "\n",
    "#improved data structures - numpy & pandas\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRISP-DM Task: Data Preparation\n",
      "Task 1: Read-in a text-based document, aka \"establishing the corpus\n",
      "Pandas File I/O Example - csv read\n",
      "                                                text  Unnamed: 1\n",
      "0  MY BANK is always good to me I have banked wit...         NaN\n",
      "1  MY BANK is the best for me They help people wh...         NaN\n",
      "2  MY BANK has been 100 percent on top on any ban...         NaN\n",
      "3  Absolutely no problems with them Everything ha...         NaN\n",
      "4  Absolutely They are efficient courteous and he...         NaN\n"
     ]
    }
   ],
   "source": [
    "#open python standard data stream to read lins into a list data structure.  \n",
    "with open(\"C:\\\\Users\\\\JTB Ventures LLC\\\\Documents\\\\GitHub\\\\ODSA-PythonAdvModels\\\\Data\\\\text8\", \"r\") as f:\n",
    "    raw_corpus = []\n",
    "    for item in f:\n",
    "        raw_corpus.append(item) #import all lines of text into python list\n",
    "#print(raw_corpus)\n",
    "\n",
    "#CRISP-DM Task: Data Preparation \n",
    "#import favorite text-based dataset for analysis using pandas dataframe - compatible w/scikit-learn\n",
    "def read_text(path):\n",
    "    print(\"Pandas File I/O Example - csv read\")\n",
    "    text=pd.read_csv(path) #import to pandas DataFrame\n",
    "    return text #return pandas dataframe type\n",
    "\n",
    "print('CRISP-DM Task: Data Preparation')\n",
    "print('Task 1: Read-in a text-based document, aka \"establishing the corpus')\n",
    "documents = read_text(\"C:\\\\Users\\\\JTB Ventures LLC\\\\Documents\\\\GitHub\\\\ODSA-PythonAdvModels\\\\Data\\\\text8\") #Relative path - to tabular csv file \n",
    "print(documents.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def preprocess_text(corpus):\\n    i = 0\\n    corpus2 = []\\n    for corpu in corpus:  #iterate through rows in dataframe\\n        line = strip_punctuation(corpu)\\n        line = strip_non_alphanum(line)\\n        line = strip_numeric(line)\\n        line = strip_multiple_whitespaces(corpu)\\n        line = remove_stopwords(line)\\n        corpus2.append(line)\\n    return corpus2\\n\\n#apply preprocessing function to \"raw\" text corpus for a \"cleaned\" corpus, to use in vectorization\\nclean_corpus = preprocess_text(raw_corpus) \\n\\nstoplist = set(\\'i it\\'s\\'.split()) #remove extraneous words from analysis\\n\\n# Lowercase each document, split it by white space and filter out stopwords\\ntexts = [[word for word in document.lower().split() if word not in stoplist]\\n         for document in clean_corpus ]\\n\\n#Long list of non-distinct parsed words from doc \\nprint(texts[992]) #comment 2'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Prep - Clean-up raw corpus for analysis\n",
    "'''def preprocess_text(corpus):\n",
    "    i = 0\n",
    "    corpus2 = []\n",
    "    for corpu in corpus:  #iterate through rows in dataframe\n",
    "        line = strip_punctuation(corpu)\n",
    "        line = strip_non_alphanum(line)\n",
    "        line = strip_numeric(line)\n",
    "        line = strip_multiple_whitespaces(corpu)\n",
    "        line = remove_stopwords(line)\n",
    "        corpus2.append(line)\n",
    "    return corpus2\n",
    "\n",
    "#apply preprocessing function to \"raw\" text corpus for a \"cleaned\" corpus, to use in vectorization\n",
    "clean_corpus = preprocess_text(raw_corpus) \n",
    "\n",
    "stoplist = set('i it\\'s'.split()) #remove extraneous words from analysis\n",
    "\n",
    "# Lowercase each document, split it by white space and filter out stopwords\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "         for document in clean_corpus ]\n",
    "\n",
    "#Long list of non-distinct parsed words from doc \n",
    "print(texts[992]) #comment 2'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2: Preprocessing dataset, including stoplist, word frequencies & filters\n",
      "Task 2a: Remove punctuation, non-alphanumeric and numeric characters\n",
      "Preprocessing Corpus from pandas data frame\n"
     ]
    }
   ],
   "source": [
    "#preprocess data for use in text mining/NLP - refactored for pandas dataframe\n",
    "def preprocess_text(corpus,field_name = 'text'):\n",
    "    print(\"Preprocessing Corpus from pandas data frame\")\n",
    "    for index, row in corpus.iterrows():  #iterate through rows in dataframe\n",
    "        line = row['text'].strip('\\n')\n",
    "        line = strip_punctuation(line)\n",
    "        line = strip_non_alphanum(line)\n",
    "        line = strip_numeric(line)\n",
    "        line = strip_multiple_whitespaces(line)\n",
    "        #line = strip_short(line)\n",
    "        #add cleaned text line to new dataframe\n",
    "        corpus.at[index,field_name] = line #set value at row/column in corpus dataframet            \n",
    "    return corpus\n",
    "\n",
    "print('Task 2: Preprocessing dataset, including stoplist, word frequencies & filters')\n",
    "print('Task 2a: Remove punctuation, non-alphanumeric and numeric characters')\n",
    "#apply preprocessing function to pandas df text field \"comment\" to create a \"raw\" text corpus\n",
    "raw_corpus = preprocess_text(documents,field_name = 'text') \n",
    "#print(raw_corpus.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], []]\n"
     ]
    }
   ],
   "source": [
    "# Data Prep - Count word frequencies\n",
    "frequency = defaultdict(int)\n",
    "\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "#Remove Words that only appear once\n",
    "processed_corpus = [\n",
    "     [token for token in text if frequency[token] > 5]\n",
    "     for text in texts\n",
    "]\n",
    "#print(processed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Prep - associate each word in the processed corpus with a unique integer ID, using the gensim.corpora.Dictionary class. \n",
    "#This dictionary defines the vocabulary of all words that our processing knows about.\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Task 2b: remove english stopwords and add additional to remove from text document')\n",
    "#set stopword list - see here for set of english \"stop words\": https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/stop_words.py\n",
    "#scikit-learn uses a stoplist \"frozenset\" - immutable python set - \"ENGLISH_STOP_WORDS\" \n",
    "#use standard english stop words, along with aditionally defined in \"Union Statement\"\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union({\"have\", \"with\", \"are\"}) \n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Prep - To infer the latent structure in our corpus we need a way to represent documents\n",
    "#that we can manipulate mathematically. One approach is to represent each document as a vector. \n",
    "#convert document into \"Bag of works\"\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]\n",
    "print(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Task 2b: create a BOW vector for use with Latent-Dirichlet-Allocation (LDA) Models, using assigned stop words')\n",
    "#Note: In Gensim Word2Vec, this is known as \"Similarity Queues\" - https://radimrehurek.com/gensim/tut3.html\n",
    "#Bag-of-words is one choice - Gensim includes: Matrix-Market, LSI,SVMlight, LDA-C, GivvsLDA++, etc\n",
    "\n",
    "#Convert a collection of text documents to a matrix of token counts - \"bag-of-words\", unless otherwise specified\n",
    "bow_vector = CountVectorizer(stop_words=stop_words)\n",
    "\n",
    "#\"Comment\" column is the 6th column in the dataset - index \"5\" in dataframe\n",
    "value_list = [row[0] for row in raw_corpus.itertuples(index=False, name=None)]\n",
    "#print(value_list[0:3])\n",
    "#create term-document matrix and and place all relevant terms in vocabulary/dictionary\n",
    "bow = bow_vector.fit_transform(value_list)\n",
    "#dictionary stored in the vocabulary_ variable of the bow object \n",
    "print(bow_vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that we have vectorized our corpus we can begin to transform it using models. \n",
    "#We use model as an abstract term referring to a transformation from one document representation to another. \n",
    "#In gensim, documents are represented as vectors so a model can be thought of as a transformation between two vector spaces. \n",
    "#The details of this transformation are learned from the training corpus. '''\n",
    "\n",
    "#One simple example of a model is tf-idf. The tf-idf model transforms vectors from the \n",
    "#bag-of-words representation to a vector space, where the frequency counts are weighted according \n",
    "#to the relative rarity of each word in the corpus.\n",
    "\n",
    "# train the model using tf-idf model\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "# transform the \"system minors\" string\n",
    "tfidf2 = tfidf[dictionary.doc2bow(\"ATM deposit\".lower().split())]\n",
    "print(tfidf2) #first entry is the token ID and the second entry is the tf-idf weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create lda model from processed corpus/tfidf transformation \n",
    "#\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "lda = models.LdaModel(corpus_tfidf, \n",
    "                          id2word=dictionary,\n",
    "                          iterations=1000,\n",
    "                          num_topics=10)\n",
    "print(lda)\n",
    "#Print Topics produced by LDA model\n",
    "for i in range(0, lda.num_topics-1):\n",
    "    print(\"topic \" + str(i) + \":\" + lda.print_topic(i))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global variable that work as model parameters - adjust for model performance \"fine-tuning\"\n",
    "n_samples = 2000 #sample size\n",
    "n_features = 1000 #name/entity recgonition & group selection (vectors)\n",
    "n_components = 10 #themes\n",
    "n_top_words = 10 #words per theme\n",
    "\n",
    "print('CRISP-DM Task: Model Building')\n",
    "print(\"Fitting LDA models with n_topic=%d, n_samples=%d and n_features=%d...\" % (n_components, n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)  #scikit-learn library\n",
    "lda.fit(bow)\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "bow_feature_names = bow_vector.get_feature_names()\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "print_top_words(lda, bow_feature_names, n_top_words) #note the improvement over last week's model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Evaluation - LDA topic Coherence - Closer to 0, the better\n",
    "#for additional measures, see: https://radimrehurek.com/gensim/models/coherencemodel.html\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "cm = CoherenceModel(model=lda, corpus=corpus_tfidf, coherence='u_mass')\n",
    "print(cm.get_coherence())  # get coherence value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare vs. topics generated by LSI model\n",
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=10)  # initialize an LSI transformation\n",
    "corpus_lsi = lsi[corpus_tfidf]  # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi\n",
    "lsi.print_topics(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#other available transformations located at: https://radimrehurek.com/gensim/tut2.html\n",
    "print('CRISP-DM Task: Model Evaluation ')\n",
    "print('Step1 : get test topics then score them vs. current LDA model')\n",
    "train, test = train_test_split(list(documents['text'].values), test_size = 0.2)\n",
    "test_vector = bow_vector.fit_transform(test)\n",
    "test_lda = lda.fit(test_vector)  #fit text vector within existing model - transpose to work\n",
    "#Calculate approximate log-likelihood as score.\n",
    "print(test_lda.score(test_vector)) #not meaningful in itself - compare vs. re-run models (closer to 0, the better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Step 2: Use formal model evaluation stats, such as \"perplexity\" from scikit-learn library') \n",
    "# create test/train text documents to evaluate model \n",
    "vectoriser = CountVectorizer(stop_words = 'english', max_features=500)  #max features must be less that \"n_features\" variable!\n",
    "doc_train = vectoriser.fit_transform(train)\n",
    "features = vectoriser.get_feature_names()\n",
    "doc_test = vectoriser.fit_transform(test)\n",
    "news_lda = lda.fit(doc_train)\n",
    "print(news_lda.perplexity(doc_test)) # lower the perplexity, the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
